<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Gaussian Process代码理解 | Lan&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="今天看了大神Nando de Freitas在UBC讲的machine learning 2013年春季课程，关于高斯过程的视频，对GP有了初步的理解。课上他给出了一段GP的代码。看见python代码就激动肯定是一种病，哈哈。逐步分析一下：
12# This is the true unknown function we are trying to approximatef = lambda x:">
<meta property="og:type" content="article">
<meta property="og:title" content="Gaussian Process代码理解">
<meta property="og:url" content="http://lan2720.github.io/2015/07/30/Gaussian-Process代码理解/index.html">
<meta property="og:site_name" content="Lan's Blog">
<meta property="og:description" content="今天看了大神Nando de Freitas在UBC讲的machine learning 2013年春季课程，关于高斯过程的视频，对GP有了初步的理解。课上他给出了一段GP的代码。看见python代码就激动肯定是一种病，哈哈。逐步分析一下：
12# This is the true unknown function we are trying to approximatef = lambda x:">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/901f9a6fgw1eukvlj5amrj20e603owel.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/901f9a6fgw1eukwkdcpo5j206b01m743.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/901f9a6fgw1eukv2actpwj20j704hmxh.jpg">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Gaussian Process代码理解">
<meta name="twitter:description" content="今天看了大神Nando de Freitas在UBC讲的machine learning 2013年春季课程，关于高斯过程的视频，对GP有了初步的理解。课上他给出了一段GP的代码。看见python代码就激动肯定是一种病，哈哈。逐步分析一下：
12# This is the true unknown function we are trying to approximatef = lambda x:">
  
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="undefined" class="js-avatar" style="width: 100%;height: 100%;opacity: 1;">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Lan</a></h1>
		</hgroup>

		
		<p class="header-subtitle">属性GEEK, 冷静思考</p>
		

		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
						</div>
					</nav>
				</section>
				
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Lan</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="undefined" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Lan</h1>
			</hgroup>
			
			<p class="header-subtitle">属性GEEK, 冷静思考</p>
			
			<nav class="header-menu">
				<ul>
				
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-Gaussian-Process代码理解" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/30/Gaussian-Process代码理解/" class="article-date">
  	<time datetime="2015-07-30T12:03:10.000Z" itemprop="datePublished">2015-07-30</time>
</a>
    </div>
  
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Gaussian Process代码理解
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machinelearning/">machinelearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/自动寻参/">自动寻参</a></li></ul>
	</div>

        

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天看了大神Nando de Freitas在UBC讲的machine learning 2013年春季课程，关于高斯过程的视频，对GP有了初步的理解。<br>课上他给出了一段GP的代码。看见python代码就激动肯定是一种病，哈哈。<br>逐步分析一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is the true unknown function we are trying to approximate</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: np.sin(<span class="number">0.9</span>*x).flatten()</span><br></pre></td></tr></table></figure>
<p>这是我们要不断去近似的一个方程。对于深度学习的自动寻参而言，我们给定几组参数组合，然后得到这几组参数对应的训练效果，那么可以根据这几组已知的结果，去预测在整个参数空间范围内，参数组合会有怎样一个训练结果趋势。这个结果趋势就是通过GP模拟出的。<br>其中隐含了一个信息：所以可能的参数组合他们所得出的训练结果真实值。但是这个真实值我们是得不到的，如果得到了我们就不用费劲，直接在这些结果中找到训练结果最好的那组参数就万事大吉了。但是，事与愿违，既然不能得到这样一个潜在的真实值，我们只能通过贝爷的方法去模拟出一个大致接近真实值的值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the kernel</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="string">""" GP squared exponential kernel """</span></span><br><span class="line">    kernelParameter = <span class="number">0.1</span></span><br><span class="line">    sqdist = np.sum(a**<span class="number">2</span>,<span class="number">1</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>) + np.sum(b**<span class="number">2</span>,<span class="number">1</span>) - <span class="number">2</span>*np.dot(a, b.T)</span><br><span class="line">    <span class="keyword">return</span> np.exp(-.<span class="number">5</span> * (<span class="number">1</span>/kernelParameter) * sqdist)</span><br></pre></td></tr></table></figure></p>
<p>这里定义了一个核方法。这个核方法是用来计算K的，K就是两个变量a，b的协方差矩阵。<br>核方法有很多，对应到不同领域又有处理文本的，处理图像音频的核方程。这里只是简单的拿一个核作为例子进行讲解。</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">10</span>         <span class="comment"># number of training points.</span></span><br><span class="line">n = <span class="number">50</span>         <span class="comment"># number of test points.</span></span><br><span class="line">s = <span class="number">0.00005</span>    <span class="comment"># noise variance.</span></span><br></pre></td></tr></table></figure>
<p>这里给出10组训练点。最后要预测50个测试点。并且在y值上加上噪音。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sample some input points and noisy versions of the function evaluated at</span></span><br><span class="line"><span class="comment"># these points. </span></span><br><span class="line">X = np.random.uniform(-<span class="number">5</span>, <span class="number">5</span>, size=(N,<span class="number">1</span>))</span><br><span class="line">y = f(X) + s*np.random.randn(N)</span><br></pre></td></tr></table></figure>
<p>生成训练点的坐标。训练点得到的观测值也可能是不准确的，所以要加上一些噪音。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">K = kernel(X, X)</span><br><span class="line">L = np.linalg.cholesky(K + s*np.eye(N))</span><br></pre></td></tr></table></figure>
<p>生成训练点的协方差矩阵K和cholesky分解后的L。<br>K和L存在如下关系，近似于开方了。<br>$K = L\cdot L^T$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># points we're going to make predictions at.</span></span><br><span class="line">Xtest = np.linspace(-<span class="number">5</span>, <span class="number">5</span>, n).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>生成测试点的坐标，而不知道这些测试点的y值，所以通过GP来预测这些y值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute the mean at our test points.</span></span><br><span class="line">Lk = np.linalg.solve(L, kernel(X, Xtest))</span><br><span class="line">mu = np.dot(Lk.T, np.linalg.solve(L, y))</span><br></pre></td></tr></table></figure>
<p>计算测试点的y值的均值（按公式）。<br><img src="http://ww1.sinaimg.cn/large/901f9a6fgw1eukvlj5amrj20e603owel.jpg" alt=""><br><img src="http://ww2.sinaimg.cn/large/901f9a6fgw1eukwkdcpo5j206b01m743.jpg" alt=""><br>将上式中$K<em>y$带换成$L\cdot L^T$。<br>得到$\mu</em>\star=k^T<em>\star L^{-T}L^{-1}y$<br>进一步将$k^T</em>\star L^{-T}$合并，得到$\mu<em>\star=(L^{-1}k</em>\star)^T L^{-1}y$<br>其中$L^{-1}k_\star = solve(L, k)$，$L^{-1}y = solve(L, y)$<br>因此就得到上面两行代码的结果。</p>
<p><img src="http://ww2.sinaimg.cn/large/901f9a6fgw1eukv2actpwj20j704hmxh.jpg" alt="first correct"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute the variance at our test points.</span></span><br><span class="line">K_ = kernel(Xtest, Xtest)</span><br><span class="line">s2 = np.diag(K_) - np.sum(Lk**<span class="number">2</span>, axis=<span class="number">0</span>)</span><br><span class="line">s = np.sqrt(s2)</span><br></pre></td></tr></table></figure>
<p>计算测试点的变动范围$\sigma$。<br>K<em>表示的是$K</em> {\star\star}$矩阵。<br>(还没弄清楚)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PLOTS:</span></span><br><span class="line">pl.figure(<span class="number">1</span>)</span><br><span class="line">pl.clf()</span><br><span class="line">pl.plot(X, y, <span class="string">'r+'</span>, ms=<span class="number">20</span>)</span><br><span class="line">pl.plot(Xtest, f(Xtest), <span class="string">'b-'</span>)</span><br></pre></td></tr></table></figure>
<p>画出测试点和测试点的真实y值（f(Xtest)）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pl.plot(Xtest, mu, <span class="string">'r--'</span>, lw=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>画测试点的均值（预测y值）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># draw samples from the prior at our test points.</span></span><br><span class="line">L = np.linalg.cholesky(K_ + <span class="number">1e-6</span>*np.eye(n))</span><br><span class="line">f_prior = np.dot(L, np.random.normal(size=(n,<span class="number">10</span>)))</span><br><span class="line">pl.figure(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>画出测试点的先验值。<br>因为$y^i \sim N(0, K)$，所以$y<em>{prior}=0 + N(0, I)\cdot L</em>{\star\star}$<br>从而得到了先验值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># draw samples from the posterior at our test points.</span></span><br><span class="line">L = np.linalg.cholesky(K_ + <span class="number">1e-6</span>*np.eye(n) - np.dot(Lk.T, Lk))</span><br><span class="line">f_post = mu.reshape(-<span class="number">1</span>,<span class="number">1</span>) + np.dot(L, np.random.normal(size=(n,<span class="number">10</span>)))</span><br><span class="line">pl.figure(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>得到测试点的后验值。<br>$f_{post} = \mu + N(0, I)\cdot L$<br>（这里的$L$还没搞清楚）</p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2015/07/23/softmax-regression-vs-logistic-regression/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">softmax regression vs logistic regression</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
</article>






</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2015 Lan
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    

<script>
	var yiliaConfig = {
		fancybox: undefined,
		mathjax: undefined,
		animate: undefined,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: undefined
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>






  </div>
</body>
</html>