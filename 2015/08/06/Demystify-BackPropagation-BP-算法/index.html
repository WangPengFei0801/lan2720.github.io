<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Demystify BackPropagation(BP)算法 | Lan&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="前言以前也听别人喊BP算法BP算法的，一直以为是个多么高深莫测的算法呢，最近看了很多资料之后，觉得BP思想一开始接触确实不容易马上理解，但是在知识积累到一定程度（其实也就几天的间隔）再回头看时，发现之前难于理解的公式现在恍然大悟了。就像NeuralNetworksAndDeepLearning的作者所言：

Be warned, though: you shouldn’t expect to in">
<meta property="og:type" content="article">
<meta property="og:title" content="Demystify BackPropagation(BP)算法">
<meta property="og:url" content="http://lan2720.github.io/2015/08/06/Demystify-BackPropagation-BP-算法/index.html">
<meta property="og:site_name" content="Lan's Blog">
<meta property="og:description" content="前言以前也听别人喊BP算法BP算法的，一直以为是个多么高深莫测的算法呢，最近看了很多资料之后，觉得BP思想一开始接触确实不容易马上理解，但是在知识积累到一定程度（其实也就几天的间隔）再回头看时，发现之前难于理解的公式现在恍然大悟了。就像NeuralNetworksAndDeepLearning的作者所言：

Be warned, though: you shouldn’t expect to in">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/901f9a6fgw1euso4ck9jvj20bm033q30.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/901f9a6fgw1euso9gietbj20dt01faa9.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/901f9a6fgw1eusplyx5p9j205702v746.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/901f9a6fgw1eusqs5jrulj207302zglm.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/901f9a6fgw1eusqoxaj4kj20gp01iglv.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/901f9a6fgw1eusquah8gzj20be02y0st.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/901f9a6fgw1eusqya1oj4j207802adfq.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/901f9a6fgw1eusrfk657ej209p01qdfu.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/901f9a6fgw1eust6u4czoj206402lmx2.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/901f9a6fgw1eusun4owxdj20bl070q3r.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/901f9a6fgw1eusurxf3aqj201401tjr6.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/901f9a6fgw1eusuu228n9j209n01rdfv.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/901f9a6fgw1eusv6jzvxbj203m02iq2t.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/901f9a6fgw1eusv745wtcj205302u0so.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/901f9a6fgw1eusvjzd6mnj205p09lq3b.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/901f9a6fgw1eusw6owst4j20as02sglt.jpg">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Demystify BackPropagation(BP)算法">
<meta name="twitter:description" content="前言以前也听别人喊BP算法BP算法的，一直以为是个多么高深莫测的算法呢，最近看了很多资料之后，觉得BP思想一开始接触确实不容易马上理解，但是在知识积累到一定程度（其实也就几天的间隔）再回头看时，发现之前难于理解的公式现在恍然大悟了。就像NeuralNetworksAndDeepLearning的作者所言：

Be warned, though: you shouldn’t expect to in">
  
    <link rel="alternative" href="/atom.xml" title="Lan&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://www.alchemyapi.com/sites/default/files/deepLearningAI500.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Lan</a></h1>
		</hgroup>

		
		<p class="header-subtitle">属性GEEK, 冷静思考</p>
		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/about">关于我</a></li>
				        
							<li><a href="/深度学习资源">深度学习资源</a></li>
				        
							<li><a href="/机器学习资源">机器学习资源</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/lan2720" title="github">github</a>
					        
								<a class="zhihu" target="_blank" href="http://www.zhihu.com/people/loafer-527" title="zhihu">zhihu</a>
					        
								<a class="douban" target="_blank" href="http://www.douban.com/people/57762260/" title="douban">douban</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/deeplearningtutorials/" style="font-size: 13.33px;">deeplearningtutorials</a><a href="/tags/java/" style="font-size: 13.33px;">java</a><a href="/tags/julyedu/" style="font-size: 10px;">julyedu</a><a href="/tags/life/" style="font-size: 10px;">life</a><a href="/tags/machinelearning/" style="font-size: 10px;">machinelearning</a><a href="/tags/neuralnetworksanddeeplearning/" style="font-size: 16.67px;">neuralnetworksanddeeplearning</a><a href="/tags/python/" style="font-size: 10px;">python</a><a href="/tags/ufldl/" style="font-size: 10px;">ufldl</a><a href="/tags/动态规划/" style="font-size: 10px;">动态规划</a><a href="/tags/学习笔记/" style="font-size: 20px;">学习笔记</a><a href="/tags/算法/" style="font-size: 10px;">算法</a><a href="/tags/自动寻参/" style="font-size: 10px;">自动寻参</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://summer-last.blogbus.com/">陈小醒的blog</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Lan</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="http://www.alchemyapi.com/sites/default/files/deepLearningAI500.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Lan</h1>
			</hgroup>
			
			<p class="header-subtitle">属性GEEK, 冷静思考</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/about">关于我</a></li>
		        
					<li><a href="/深度学习资源">深度学习资源</a></li>
		        
					<li><a href="/机器学习资源">机器学习资源</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/lan2720" title="github">github</a>
			        
						<a class="zhihu" target="_blank" href="http://www.zhihu.com/people/loafer-527" title="zhihu">zhihu</a>
			        
						<a class="douban" target="_blank" href="http://www.douban.com/people/57762260/" title="douban">douban</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-Demystify-BackPropagation-BP-算法" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/08/06/Demystify-BackPropagation-BP-算法/" class="article-date">
  	<time datetime="2015-08-06T11:48:24.000Z" itemprop="datePublished">2015-08-06</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Demystify BackPropagation(BP)算法
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/neuralnetworksanddeeplearning/">neuralnetworksanddeeplearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

        

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="前言">前言</h2><p>以前也听别人喊BP算法BP算法的，一直以为是个多么高深莫测的算法呢，最近看了很多资料之后，觉得BP思想一开始接触确实不容易马上理解，但是在知识积累到一定程度（其实也就几天的间隔）再回头看时，发现之前难于理解的公式现在恍然大悟了。就像NeuralNetworksAndDeepLearning的作者所言：</p>
<blockquote>
<p>Be warned, though: you shouldn’t expect to instantaneously assimilate the equations. Such an expectation will lead to disappointment. In fact, the backpropagation equations are so rich that understanding them well requires considerable time and patience as you gradually delve deeper into the equations.</p>
</blockquote>
<p>所以，如果你现在还不理解BP算法，没有关系，看完第一遍之后隔一个星期再看，肯定会理解，而且反复思索的过程中收获更多。</p>
<h2 id="什么是BP算法？BP算法要解决什么问题？">什么是BP算法？BP算法要解决什么问题？</h2><p>神经网络说白了就两个过程。<br>就拿监督学习来说吧，有inputs， 有label（输出的准确值）。我唯一的目标就是希望我的整个网络接收到inputs之后，输出的结果和给定的labels差距尽可能小，因为那样就说明我完成了对数据的准确预测。那么来一个新的input，我就能八九不离十的预测它应该属于哪个label。<br>有了这个终极梦想之后，我们接下来就去想如何实现梦想：</p>
<ol>
<li>确定模型：这里面就涉及到模型需要多少层，每一层的激活函数，cost function（cost function就是衡量output和y的差距的方法）等。</li>
<li>模型确定好之后，就该训练模型了，我希望经过训练之后，这个模型能完成我的梦想，哈哈。</li>
</ol>
<p>仔细想想，我们该如何训练呢？因为我们的目标是让输出和y的差距尽可能小，所以我们训练过程就是不断让cost function尽可能小，进一步就想到了梯度下降法。</p>
<p>既然是梯度下降，就要计算参数的导数。<br>这是我们的cost function。<br><img src="http://ww2.sinaimg.cn/large/901f9a6fgw1euso4ck9jvj20bm033q30.jpg" alt=""><br>希望它最小，那么在给定的y(x)和x情况下，可变的只有$a^L(x)$，又因为$a^L(x) = activation(w\cdot x+b)$，activation function也是模型确定阶段就定好的，所以可变的只有参数w和b了。<br>那么我们的目标就变成了：找到尽可能最好的w和b使得cost function最小。<br>所以，要计算导数，也就是cost function 关于w和b求偏导（多变量求导叫偏导数）。<br><img src="http://ww1.sinaimg.cn/large/901f9a6fgw1euso9gietbj20dt01faa9.jpg" alt=""></p>
<p>但是，我们仔细看看上面的那个cost function的表达式又有一个问题。神经网络的每一层都有w和b，但是cost function只衡量整个神经网络的最后一层输出$a^L(x)$和目标值$y$的差距，也就是最后一层的w和b的偏导很容易求出。对于之前的层我们怎么求cost_derivative_wrt_w呢？<br>Good question！智慧的前人也想到了这个问题，这就是发明BP算法的原因。</p>
<p>所以大数据时代，人们每天说的训练模型训练模型，其实就是一个简单的BP过程反复多次而已，并没有什么高深的“训练”。</p>
<h2 id="BP算法如何解决问题">BP算法如何解决问题</h2><p>由于我们的cost function只停留在最后一层，但是每一层的每个神经元都对最后的$a^L(x)$有贡献，那么我们就想：“如果能把前面层对cost function所做的贡献分离出来回传回去，不就能求出前面层的cost_derivative_wrt_w了吗？”。这个思路非常正确，如果早生N年，这个将cost function的一部分回传的算法就是你发明的～</p>
<h2 id="BP算法layer-to-layer公式推导">BP算法layer-to-layer公式推导</h2><p>读到这里如果没有不懂的话，我想你已经弄明白BP算法的思想了（只是还没有代码实现而已）。是不是很疑惑为何如此simple and intuitive。</p>
<blockquote>
<p>经典的东西往往是至简的。</p>
</blockquote>
<p>现在你脑子里应该还有最后一个疑惑：我们如何将前面层对最后的cost function所做的贡献分离出来回传给之前层呢？</p>
<blockquote>
<p>真相只有一个：要~分~离~，眼(gong)泪(shi)就(tui)留(dao)下去～～～</p>
</blockquote>
<p>其实只用到了微积分中最基本的链式法则，所以并不用惊慌啦。<br>具体推导看<a href="http://neuralnetworksanddeeplearning.com/chap2.html#the_four_fundamental_equations_behind_backpropagation" target="_blank" rel="external">这里</a><br>我只提取精华部分：<br>第l层的第j个neuron，它的input记为$Z_j^l$，output记为$a^l_j$<br>首先定义一个符号<img src="http://ww4.sinaimg.cn/large/901f9a6fgw1eusplyx5p9j205702v746.jpg" alt=""><br>然后我们先求$\delta_j^l$然后将$\delta_j^l$和cost_derivative_cwt_w和cost_derivative_wrt_b联系起来（如何联系后面讲解）。</p>
<h3 id="$\delta^l$的计算">$\delta^l$的计算</h3><h4 id="第一步：最后一层">第一步：最后一层</h4><p><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusqs5jrulj207302zglm.jpg" alt="">（1）<br>如果cost function选取的是mean square，那么<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusqoxaj4kj20gp01iglv.jpg" alt="">（2）<br>由于$a^L_j = activation(Z^L_j)$，如果activation function选取的是sigmoid，记做f，那么<br>sigmoid_prime = f(1 - f)<br><img src="http://ww2.sinaimg.cn/large/901f9a6fgw1eusquah8gzj20be02y0st.jpg" alt="">（3）<br>将（2）,（3）带入（1）得到如下结果，<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusqya1oj4j207802adfq.jpg" alt="">（4）<br>【注意】这是在cost function=meansquare， activation function=sigmoid的情况下得到的，其他的选择得出的公式不一样。<br>接下来考虑vectorize的事情。<br>$a^j$,$y^j$和$Z^L_j$都是vector，维度和最后一层的neuron个数相同，所以他们直接相乘是Hadamard product(element-wise)，记为<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusrfk657ej209p01qdfu.jpg" alt=""></p>
<p>到这里，我们就计算得到了$\delta_j^L$。</p>
<h4 id="第二步：中间层">第二步：中间层</h4><p>中间层计算公式如最后一层<img src="http://ww2.sinaimg.cn/large/901f9a6fgw1eust6u4czoj206402lmx2.jpg" alt="">（5）<br>关键就在于cost_derivative_wrt_a_l的计算。<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusun4owxdj20bl070q3r.jpg" alt=""><br>l+1层的$\delta^{l+1}$是由最顶层的cost function逐层传递下来的，代表了l+1层接受到的误差。再传递到l层时，l层的输出就是$(W^{l+1})^T\cdot \delta^{l+1} $，即求到了<img src="http://ww1.sinaimg.cn/large/901f9a6fgw1eusurxf3aqj201401tjr6.jpg" alt="">，带入到（5）中，<br><img src="http://ww1.sinaimg.cn/large/901f9a6fgw1eusuu228n9j209n01rdfv.jpg" alt="">（6）<br>这就是中间层的$\delta^l$的计算公式。<br>BP过程中的权重矩阵可以定义成别的形式，但是如果是$w^T$的话，就被叫做tied weights。</p>
<h3 id="与cost_derivative_wrt_w联系">与cost_derivative_wrt_w联系</h3><p>知道了$\delta ^l$最重要的就是用它来求得w和b的偏微分。<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusv6jzvxbj203m02iq2t.jpg" alt="">（7）<br><img src="http://ww2.sinaimg.cn/large/901f9a6fgw1eusv745wtcj205302u0so.jpg" alt="">（8）<br>这就是两者相联系的公式。<br>具体推导过程如下：<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusvjzd6mnj205p09lq3b.jpg" alt=""><br>(b_prime类似)<br>最后进行vectorize：只对l层而言，w就是一个二维矩阵，例如l-1层有n个neurons，l层有m个neurons，那么$w^l$就是m<em>n的矩阵。$\delta^l$是m×1的vector，$a^{l-1}$是n×1的vector，要得到m×n的matrix需要$\delta^l_j</em>(a^{l-1}_k)^T$。</p>
<h2 id="总结">总结</h2><p>在实际的应用中，比如SGD，每个mini_batch有m个training examples，更新结果就应该是对mini_batch中所有的example都做这个操作再求和取平均值进行更新<img src="http://ww4.sinaimg.cn/large/901f9a6fgw1eusw6owst4j20as02sglt.jpg" alt="">，$\eta$是learning rate。这里$\delta^{x,l}$表示第x个training_example的第l层对应的$\delta^l$，</p>
<h2 id="代码解析">代码解析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return a tuple "(nabla_b, nabla_w)" representing the</span><br><span class="line">        gradient for the cost function C_x.  "nabla_b" and</span><br><span class="line">        "nabla_w" are layer-by-layer lists of numpy arrays, similar</span><br><span class="line">        to "self.biases" and "self.weights"."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid_vec(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[-<span class="number">1</span>], y) * \</span><br><span class="line">            sigmoid_prime_vec(zs[-<span class="number">1</span>])</span><br><span class="line">        nabla_b[-<span class="number">1</span>] = delta</span><br><span class="line">        nabla_w[-<span class="number">1</span>] = np.dot(delta, activations[-<span class="number">2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            spv = sigmoid_prime_vec(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * spv</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l-<span class="number">1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br></pre></td></tr></table></figure>
<p>最后分析下BP的代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br></pre></td></tr></table></figure></p>
<p>如上所说，每一层的weights就是一个二维矩阵，一层层的二维矩阵放在一个list列表中，biases和weights都是这么存储的。<br>nabla_b和nabla_w也要和自己的妈妈有一样的存储形式，所以初始化时将每一层的matrix全设置为0，再生成每一层构成列表。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feedforward</span></span><br><span class="line">activation = x</span><br><span class="line">activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br></pre></td></tr></table></figure></p>
<p>从输入层开始进行前向传播。activations列表存放每一层输出的激励值（从input layer开始）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line"><span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">    z = np.dot(w, activation)+b</span><br><span class="line">    zs.append(z)</span><br><span class="line">    activation = sigmoid_vec(z)</span><br><span class="line">    activations.append(activation)</span><br></pre></td></tr></table></figure></p>
<p>zs也是一个列表存放每一层的输入。zs经过neuron的激励方程之后就得到activations(从第一个hidden layer开始)。<br>由于input layer没有激励，直接输出，所以就不存在z，只将input作为了第一层的activation。<br>接下来，for循环将每一层的w和b提取出来，b是一个vector，w是一个2d-matrix。第一层的activation已经存好了，所以从第二层开始（也就是第一个hidden layer），由上一层的activation计算这一层的z，$z^l = w\cdot a^{l-1} + b(l &gt;= 1)$，存储到zs列表中，即这一层的z已经计算好了。然后z经过neuron的激励，得到当前层的activation = sigmoid(z)，把当前层的activation也存储到activations列表中，即这一层的a也已经计算完成了。<br>然后逐层进行这个过程，一步步的向前推进，就是feedforward的过程。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># backward pass</span></span><br><span class="line">delta = self.cost_derivative(activations[-<span class="number">1</span>], y) * \</span><br><span class="line">            sigmoid_prime_vec(zs[-<span class="number">1</span>])</span><br><span class="line">nabla_b[-<span class="number">1</span>] = delta</span><br><span class="line">nabla_w[-<span class="number">1</span>] = np.dot(delta, activations[-<span class="number">2</span>].transpose())</span><br></pre></td></tr></table></figure></p>
<p>接下来就是反向的BP训练过程。<br>首先是最后一层的nabla_b和nabla_w的计算。那么要先算出$\delta^L$，由公式（4）可以写出<br><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delta = self.cost_derivative<span class="comment">(activations[-1], y)</span> * sigmoid_prime_vec<span class="comment">(zs[-1])</span></span><br></pre></td></tr></table></figure></p>
<p>其中cost_derivative只是做了减法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">    <span class="string">"""Return the vector of partial derivatives \partial C_x /</span><br><span class="line">    \partial a for the output activations."""</span></span><br><span class="line">    <span class="keyword">return</span> (output_activations-y)</span><br></pre></td></tr></table></figure></p>
<p>然后根据公式（7）（8）写出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nabla_b[-<span class="number">1</span>] = delta</span><br><span class="line">nabla_w[-<span class="number">1</span>] = np.dot(delta, activations[-<span class="number">2</span>].transpose())</span><br></pre></td></tr></table></figure></p>
<p>大功告成！最后看中间层的偏微分如何计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">     z = zs[-l]</span><br><span class="line">     spv = sigmoid_prime_vec(z)</span><br><span class="line">     delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * spv</span><br><span class="line">     nabla_b[-l] = delta</span><br><span class="line">     nabla_w[-l] = np.dot(delta, activations[-l-<span class="number">1</span>].transpose())</span><br><span class="line"> <span class="keyword">return</span> (nabla_b, nabla_w)</span><br></pre></td></tr></table></figure></p>
<p>因为l是从2开始的，所以<code>z = zs[-l]</code>实际上是从倒数第二层开始往后的（里面变量是l不是1）。先根据公式（6）计算$\delta^l = ((w^{l+1})^T\cdot \delta^{l+1})\cdot \sigma^{-1}(z^l)$，最后根据公式(7)(8)得到nabla。<br>就这样从输出层，逐层回退，逐步得到所有层的w和b的偏导数。</p>
<h2 id="问题">问题</h2><p>经过了几个公式的推导，目前的模型还存在一些问题。</p>
<ol>
<li>公式（8）可以看出nabla_w的大小和activation的值的大小有关，如果整个网络大部分的神经元被抑制，activation较小，那么nabla_w就会很小，更新慢。</li>
<li>公式（6）中，$\sigma ^{-1}(z)$，由于sigmoid函数自身的形态，当它的输入z接近0或1时，函数的曲线非常平，意味着这两块区域的导数都很小，从而也会使我们的更新训练速度变慢。</li>
</ol>
<h2 id="参考资料">参考资料</h2><p>neuronnetworksanddeeplearning-chapter2<br><a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">http://neuralnetworksanddeeplearning.com/chap2.html</a></p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2015/07/30/Gaussian-Process代码理解/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Gaussian Process代码理解</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
</article>


<div class="share">
	<!-- JiaThis Button BEGIN -->
	<div class="jiathis_style">
		<span class="jiathis_txt">分享到：</span>
		<a class="jiathis_button_tsina"></a>
		<a class="jiathis_button_cqq"></a>
		<a class="jiathis_button_douban"></a>
		<a class="jiathis_button_weixin"></a>
		<a class="jiathis_button_tumblr"></a>
		<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	</div>
	<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1405949716054953" charset="utf-8"></script>
	<!-- JiaThis Button END -->
</div>



<div class="duoshuo">
    <!-- 多说评论框 start -->
    <div class="ds-thread" data-thread-key="Demystify-BackPropagation-BP-算法" data-title="Demystify BackPropagation(BP)算法" data-url="http://lan2720.github.io/2015/08/06/Demystify-BackPropagation-BP-算法/"></div>
    <!-- 多说评论框 end -->
    <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
    <script type="text/javascript">
    var duoshuoQuery = {short_name:"lan-blog"};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
    </script>
    <!-- 多说公共JS代码 end -->
</div>



</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2015 Lan
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>