<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Lan&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Lan's Blog">
<meta property="og:url" content="http://lan2720.github.io/index.html">
<meta property="og:site_name" content="Lan's Blog">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lan's Blog">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="Lan&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://www.alchemyapi.com/sites/default/files/deepLearningAI500.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Lan</a></h1>
		</hgroup>

		
		<p class="header-subtitle">属性GEEK, 冷静思考</p>
		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/about">关于我</a></li>
				        
							<li><a href="/深度学习资源">深度学习资源</a></li>
				        
							<li><a href="/机器学习资源">机器学习资源</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/lan2720" title="github">github</a>
					        
								<a class="zhihu" target="_blank" href="http://www.zhihu.com/people/loafer-527" title="zhihu">zhihu</a>
					        
								<a class="douban" target="_blank" href="http://www.douban.com/people/57762260/" title="douban">douban</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/deeplearningtutorials/" style="font-size: 13.33px;">deeplearningtutorials</a><a href="/tags/java/" style="font-size: 13.33px;">java</a><a href="/tags/julyedu/" style="font-size: 10px;">julyedu</a><a href="/tags/life/" style="font-size: 10px;">life</a><a href="/tags/machinelearning/" style="font-size: 10px;">machinelearning</a><a href="/tags/neuralnetworksanddeeplearning/" style="font-size: 16.67px;">neuralnetworksanddeeplearning</a><a href="/tags/python/" style="font-size: 10px;">python</a><a href="/tags/ufldl/" style="font-size: 10px;">ufldl</a><a href="/tags/动态规划/" style="font-size: 10px;">动态规划</a><a href="/tags/学习笔记/" style="font-size: 20px;">学习笔记</a><a href="/tags/算法/" style="font-size: 10px;">算法</a><a href="/tags/自动寻参/" style="font-size: 10px;">自动寻参</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://summer-last.blogbus.com/">陈小醒的blog</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Lan</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="http://www.alchemyapi.com/sites/default/files/deepLearningAI500.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Lan</h1>
			</hgroup>
			
			<p class="header-subtitle">属性GEEK, 冷静思考</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/about">关于我</a></li>
		        
					<li><a href="/深度学习资源">深度学习资源</a></li>
		        
					<li><a href="/机器学习资源">机器学习资源</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/lan2720" title="github">github</a>
			        
						<a class="zhihu" target="_blank" href="http://www.zhihu.com/people/loafer-527" title="zhihu">zhihu</a>
			        
						<a class="douban" target="_blank" href="http://www.douban.com/people/57762260/" title="douban">douban</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-Demystify-BackPropagation-BP-算法" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/08/06/Demystify-BackPropagation-BP-算法/" class="article-date">
  	<time datetime="2015-08-06T11:48:24.000Z" itemprop="datePublished">2015-08-06</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/06/Demystify-BackPropagation-BP-算法/">Demystify BackPropagation(BP)算法</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="前言">前言</h2><p>以前也听别人喊BP算法BP算法的，一直以为是个多么高深莫测的算法呢，最近看了很多资料之后，觉得BP思想一开始接触确实不容易马上理解，但是在知识积累到一定程度（其实也就几天的间隔）再回头看时，发现之前难于理解的公式现在恍然大悟了。就像NeuralNetworksAndDeepLearning的作者所言：</p>
<blockquote>
<p>Be warned, though: you shouldn’t expect to instantaneously assimilate the equations. Such an expectation will lead to disappointment. In fact, the backpropagation equations are so rich that understanding them well requires considerable time and patience as you gradually delve deeper into the equations.</p>
</blockquote>
<p>所以，如果你现在还不理解BP算法，没有关系，看完第一遍之后隔一个星期再看，肯定会理解，而且反复思索的过程中收获更多。</p>
<h2 id="什么是BP算法？BP算法要解决什么问题？">什么是BP算法？BP算法要解决什么问题？</h2><p>神经网络说白了就两个过程。<br>就拿监督学习来说吧，有inputs， 有label（输出的准确值）。我唯一的目标就是希望我的整个网络接收到inputs之后，输出的结果和给定的labels差距尽可能小，因为那样就说明我完成了对数据的准确预测。那么来一个新的input，我就能八九不离十的预测它应该属于哪个label。<br>有了这个终极梦想之后，我们接下来就去想如何实现梦想：</p>
<ol>
<li>确定模型：这里面就涉及到模型需要多少层，每一层的激活函数，cost function（cost function就是衡量output和y的差距的方法）等。</li>
<li>模型确定好之后，就该训练模型了，我希望经过训练之后，这个模型能完成我的梦想，哈哈。</li>
</ol>
<p>仔细想想，我们该如何训练呢？因为我们的目标是让输出和y的差距尽可能小，所以我们训练过程就是不断让cost function尽可能小，进一步就想到了梯度下降法。</p>
<p>既然是梯度下降，就要计算参数的导数。<br>这是我们的cost function。<br><img src="http://ww2.sinaimg.cn/large/901f9a6fgw1euso4ck9jvj20bm033q30.jpg" alt=""><br>希望它最小，那么在给定的y(x)和x情况下，可变的只有$a^L(x)$，又因为$a^L(x) = activation(w\cdot x+b)$，activation function也是模型确定阶段就定好的，所以可变的只有参数w和b了。<br>那么我们的目标就变成了：找到尽可能最好的w和b使得cost function最小。<br>所以，要计算导数，也就是cost function 关于w和b求偏导（多变量求导叫偏导数）。<br><img src="http://ww1.sinaimg.cn/large/901f9a6fgw1euso9gietbj20dt01faa9.jpg" alt=""></p>
<p>但是，我们仔细看看上面的那个cost function的表达式又有一个问题。神经网络的每一层都有w和b，但是cost function只衡量整个神经网络的最后一层输出$a^L(x)$和目标值$y$的差距，也就是最后一层的w和b的偏导很容易求出。对于之前的层我们怎么求cost_derivative_wrt_w呢？<br>Good question！智慧的前人也想到了这个问题，这就是发明BP算法的原因。</p>
<p>所以大数据时代，人们每天说的训练模型训练模型，其实就是一个简单的BP过程反复多次而已，并没有什么高深的“训练”。</p>
<h2 id="BP算法如何解决问题">BP算法如何解决问题</h2><p>由于我们的cost function只停留在最后一层，但是每一层的每个神经元都对最后的$a^L(x)$有贡献，那么我们就想：“如果能把前面层对cost function所做的贡献分离出来回传回去，不就能求出前面层的cost_derivative_wrt_w了吗？”。这个思路非常正确，如果早生N年，这个将cost function的一部分回传的算法就是你发明的～</p>
<h2 id="BP算法layer-to-layer公式推导">BP算法layer-to-layer公式推导</h2><p>读到这里如果没有不懂的话，我想你已经弄明白BP算法的思想了（只是还没有代码实现而已）。是不是很疑惑为何如此simple and intuitive。</p>
<blockquote>
<p>经典的东西往往是至简的。</p>
</blockquote>
<p>现在你脑子里应该还有最后一个疑惑：我们如何将前面层对最后的cost function所做的贡献分离出来回传给之前层呢？</p>
<blockquote>
<p>真相只有一个：要~分~离~，眼(gong)泪(shi)就(tui)留(dao)下去～～～</p>
</blockquote>
<p>其实只用到了微积分中最基本的链式法则，所以并不用惊慌啦。<br>具体推导看<a href="http://neuralnetworksanddeeplearning.com/chap2.html#the_four_fundamental_equations_behind_backpropagation" target="_blank" rel="external">这里</a><br>我只提取精华部分：<br>第l层的第j个neuron，它的input记为$Z_j^l$，output记为$a^l_j$<br>首先定义一个符号<img src="http://ww4.sinaimg.cn/large/901f9a6fgw1eusplyx5p9j205702v746.jpg" alt=""><br>然后我们先求$\delta_j^l$然后将$\delta_j^l$和cost_derivative_cwt_w和cost_derivative_wrt_b联系起来（如何联系后面讲解）。</p>
<h3 id="$\delta^l$的计算">$\delta^l$的计算</h3><h4 id="第一步：最后一层">第一步：最后一层</h4><p><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusqs5jrulj207302zglm.jpg" alt="">（1）<br>如果cost function选取的是mean square，那么<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusqoxaj4kj20gp01iglv.jpg" alt="">（2）<br>由于$a^L_j = activation(Z^L_j)$，如果activation function选取的是sigmoid，记做f，那么<br>sigmoid_prime = f(1 - f)<br><img src="http://ww2.sinaimg.cn/large/901f9a6fgw1eusquah8gzj20be02y0st.jpg" alt="">（3）<br>将（2）,（3）带入（1）得到如下结果，<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusqya1oj4j207802adfq.jpg" alt="">（4）<br>【注意】这是在cost function=meansquare， activation function=sigmoid的情况下得到的，其他的选择得出的公式不一样。<br>接下来考虑vectorize的事情。<br>$a^j$,$y^j$和$Z^L_j$都是vector，维度和最后一层的neuron个数相同，所以他们直接相乘是Hadamard product(element-wise)，记为<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusrfk657ej209p01qdfu.jpg" alt=""></p>
<p>到这里，我们就计算得到了$\delta_j^L$。</p>
<h4 id="第二步：中间层">第二步：中间层</h4><p>中间层计算公式如最后一层<img src="http://ww2.sinaimg.cn/large/901f9a6fgw1eust6u4czoj206402lmx2.jpg" alt="">（5）<br>关键就在于cost_derivative_wrt_a_l的计算。<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusun4owxdj20bl070q3r.jpg" alt=""><br>l+1层的$\delta^{l+1}$是由最顶层的cost function逐层传递下来的，代表了l+1层接受到的误差。再传递到l层时，l层的输出就是$(W^{l+1})^T\cdot \delta^{l+1} $，即求到了<img src="http://ww1.sinaimg.cn/large/901f9a6fgw1eusurxf3aqj201401tjr6.jpg" alt="">，带入到（5）中，<br><img src="http://ww1.sinaimg.cn/large/901f9a6fgw1eusuu228n9j209n01rdfv.jpg" alt="">（6）<br>这就是中间层的$\delta^l$的计算公式。<br>BP过程中的权重矩阵可以定义成别的形式，但是如果是$w^T$的话，就被叫做tied weights。</p>
<h3 id="与cost_derivative_wrt_w联系">与cost_derivative_wrt_w联系</h3><p>知道了$\delta ^l$最重要的就是用它来求得w和b的偏微分。<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusv6jzvxbj203m02iq2t.jpg" alt="">（7）<br><img src="http://ww2.sinaimg.cn/large/901f9a6fgw1eusv745wtcj205302u0so.jpg" alt="">（8）<br>这就是两者相联系的公式。<br>具体推导过程如下：<br><img src="http://ww3.sinaimg.cn/large/901f9a6fgw1eusvjzd6mnj205p09lq3b.jpg" alt=""><br>(b_prime类似)<br>最后进行vectorize：只对l层而言，w就是一个二维矩阵，例如l-1层有n个neurons，l层有m个neurons，那么$w^l$就是m<em>n的矩阵。$\delta^l$是m×1的vector，$a^{l-1}$是n×1的vector，要得到m×n的matrix需要$\delta^l_j</em>(a^{l-1}_k)^T$。</p>
<h2 id="总结">总结</h2><p>在实际的应用中，比如SGD，每个mini_batch有m个training examples，更新结果就应该是对mini_batch中所有的example都做这个操作再求和取平均值进行更新<img src="http://ww4.sinaimg.cn/large/901f9a6fgw1eusw6owst4j20as02sglt.jpg" alt="">，$\eta$是learning rate。这里$\delta^{x,l}$表示第x个training_example的第l层对应的$\delta^l$，</p>
<h2 id="代码解析">代码解析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return a tuple "(nabla_b, nabla_w)" representing the</span><br><span class="line">        gradient for the cost function C_x.  "nabla_b" and</span><br><span class="line">        "nabla_w" are layer-by-layer lists of numpy arrays, similar</span><br><span class="line">        to "self.biases" and "self.weights"."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid_vec(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[-<span class="number">1</span>], y) * \</span><br><span class="line">            sigmoid_prime_vec(zs[-<span class="number">1</span>])</span><br><span class="line">        nabla_b[-<span class="number">1</span>] = delta</span><br><span class="line">        nabla_w[-<span class="number">1</span>] = np.dot(delta, activations[-<span class="number">2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            spv = sigmoid_prime_vec(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * spv</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l-<span class="number">1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br></pre></td></tr></table></figure>
<p>最后分析下BP的代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br></pre></td></tr></table></figure></p>
<p>如上所说，每一层的weights就是一个二维矩阵，一层层的二维矩阵放在一个list列表中，biases和weights都是这么存储的。<br>nabla_b和nabla_w也要和自己的妈妈有一样的存储形式，所以初始化时将每一层的matrix全设置为0，再生成每一层构成列表。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feedforward</span></span><br><span class="line">activation = x</span><br><span class="line">activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br></pre></td></tr></table></figure></p>
<p>从输入层开始进行前向传播。activations列表存放每一层输出的激励值（从input layer开始）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line"><span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">    z = np.dot(w, activation)+b</span><br><span class="line">    zs.append(z)</span><br><span class="line">    activation = sigmoid_vec(z)</span><br><span class="line">    activations.append(activation)</span><br></pre></td></tr></table></figure></p>
<p>zs也是一个列表存放每一层的输入。zs经过neuron的激励方程之后就得到activations(从第一个hidden layer开始)。<br>由于input layer没有激励，直接输出，所以就不存在z，只将input作为了第一层的activation。<br>接下来，for循环将每一层的w和b提取出来，b是一个vector，w是一个2d-matrix。第一层的activation已经存好了，所以从第二层开始（也就是第一个hidden layer），由上一层的activation计算这一层的z，$z^l = w\cdot a^{l-1} + b(l &gt;= 1)$，存储到zs列表中，即这一层的z已经计算好了。然后z经过neuron的激励，得到当前层的activation = sigmoid(z)，把当前层的activation也存储到activations列表中，即这一层的a也已经计算完成了。<br>然后逐层进行这个过程，一步步的向前推进，就是feedforward的过程。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># backward pass</span></span><br><span class="line">delta = self.cost_derivative(activations[-<span class="number">1</span>], y) * \</span><br><span class="line">            sigmoid_prime_vec(zs[-<span class="number">1</span>])</span><br><span class="line">nabla_b[-<span class="number">1</span>] = delta</span><br><span class="line">nabla_w[-<span class="number">1</span>] = np.dot(delta, activations[-<span class="number">2</span>].transpose())</span><br></pre></td></tr></table></figure></p>
<p>接下来就是反向的BP训练过程。<br>首先是最后一层的nabla_b和nabla_w的计算。那么要先算出$\delta^L$，由公式（4）可以写出<br><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delta = self.cost_derivative<span class="comment">(activations[-1], y)</span> * sigmoid_prime_vec<span class="comment">(zs[-1])</span></span><br></pre></td></tr></table></figure></p>
<p>其中cost_derivative只是做了减法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">    <span class="string">"""Return the vector of partial derivatives \partial C_x /</span><br><span class="line">    \partial a for the output activations."""</span></span><br><span class="line">    <span class="keyword">return</span> (output_activations-y)</span><br></pre></td></tr></table></figure></p>
<p>然后根据公式（7）（8）写出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nabla_b[-<span class="number">1</span>] = delta</span><br><span class="line">nabla_w[-<span class="number">1</span>] = np.dot(delta, activations[-<span class="number">2</span>].transpose())</span><br></pre></td></tr></table></figure></p>
<p>大功告成！最后看中间层的偏微分如何计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">     z = zs[-l]</span><br><span class="line">     spv = sigmoid_prime_vec(z)</span><br><span class="line">     delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * spv</span><br><span class="line">     nabla_b[-l] = delta</span><br><span class="line">     nabla_w[-l] = np.dot(delta, activations[-l-<span class="number">1</span>].transpose())</span><br><span class="line"> <span class="keyword">return</span> (nabla_b, nabla_w)</span><br></pre></td></tr></table></figure></p>
<p>因为l是从2开始的，所以<code>z = zs[-l]</code>实际上是从倒数第二层开始往后的（里面变量是l不是1）。先根据公式（6）计算$\delta^l = ((w^{l+1})^T\cdot \delta^{l+1})\cdot \sigma^{-1}(z^l)$，最后根据公式(7)(8)得到nabla。<br>就这样从输出层，逐层回退，逐步得到所有层的w和b的偏导数。</p>
<h2 id="问题">问题</h2><p>经过了几个公式的推导，目前的模型还存在一些问题。</p>
<ol>
<li>公式（8）可以看出nabla_w的大小和activation的值的大小有关，如果整个网络大部分的神经元被抑制，activation较小，那么nabla_w就会很小，更新慢。</li>
<li>公式（6）中，$\sigma ^{-1}(z)$，由于sigmoid函数自身的形态，当它的输入z接近0或1时，函数的曲线非常平，意味着这两块区域的导数都很小，从而也会使我们的更新训练速度变慢。</li>
</ol>
<h2 id="参考资料">参考资料</h2><p>neuronnetworksanddeeplearning-chapter2<br><a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">http://neuralnetworksanddeeplearning.com/chap2.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/neuralnetworksanddeeplearning/">neuralnetworksanddeeplearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-Gaussian-Process代码理解" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/30/Gaussian-Process代码理解/" class="article-date">
  	<time datetime="2015-07-30T12:03:10.000Z" itemprop="datePublished">2015-07-30</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/30/Gaussian-Process代码理解/">Gaussian Process代码理解</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天看了大神Nando de Freitas在UBC讲的machine learning 2013年春季课程，关于高斯过程的视频，对GP有了初步的理解。<br>课上他给出了一段GP的代码。<br>逐步分析一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is the true unknown function we are trying to approximate</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: np.sin(<span class="number">0.9</span>*x).flatten()</span><br></pre></td></tr></table></figure>
<p>这是我们要不断去近似的一个方程。对于深度学习的自动寻参而言，我们给定几组参数组合，然后得到这几组参数对应的训练效果，那么可以根据这几组已知的结果，去预测在整个参数空间范围内，参数组合会有怎样一个训练结果趋势。这个结果趋势就是通过GP模拟出的。<br>其中隐含了一个信息：所以可能的参数组合他们所得出的训练结果真实值。但是这个真实值我们是得不到的，如果得到了我们就不用费劲，直接在这些结果中找到训练结果最好的那组参数就万事大吉了。但是，事与愿违，既然不能得到这样一个潜在的真实值，我们只能通过贝爷的方法去模拟出一个大致接近真实值的值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the kernel</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="string">""" GP squared exponential kernel """</span></span><br><span class="line">    kernelParameter = <span class="number">0.1</span></span><br><span class="line">    sqdist = np.sum(a**<span class="number">2</span>,<span class="number">1</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>) + np.sum(b**<span class="number">2</span>,<span class="number">1</span>) - <span class="number">2</span>*np.dot(a, b.T)</span><br><span class="line">    <span class="keyword">return</span> np.exp(-.<span class="number">5</span> * (<span class="number">1</span>/kernelParameter) * sqdist)</span><br></pre></td></tr></table></figure></p>
<p>这里定义了一个核方法。这个核方法是用来计算K的，K就是两个变量a，b的协方差矩阵。<br>核方法有很多，对应到不同领域又有处理文本的，处理图像音频的核方程。这里只是简单的拿一个核作为例子进行讲解。<br>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machinelearning/">machinelearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/自动寻参/">自动寻参</a></li></ul>
	</div>

      

      
        <p class="article-more-link">
          <a  href="/2015/07/30/Gaussian-Process代码理解/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-softmax-regression-vs-logistic-regression" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/23/softmax-regression-vs-logistic-regression/" class="article-date">
  	<time datetime="2015-07-23T13:52:36.000Z" itemprop="datePublished">2015-07-23</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/23/softmax-regression-vs-logistic-regression/">softmax regression vs logistic regression</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="基础知识">基础知识</h2><p>logistic regression是个二分类问题。比如，预测病人的肿瘤是恶性（malignant）还是良性（benign）的情况。<br>sofxmax　regression是个多分类问题，类标签$y$可以取两个以上的值。诸如MNIST手写数字分类，辨识10个不同的单个数字。</p>
<h2 id="什么时候用哪个">什么时候用哪个</h2><p>一开始我也很疑惑什么时候用logistic regression什么时候又该用sofxmax regression呢。<a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92#Softmax_.E5.9B.9E.E5.BD.92_vs._k_.E4.B8.AA.E4.BA.8C.E5.85.83.E5.88.86.E7.B1.BB.E5.99.A8">ufldl</a>给出了一个很简单的方法。</p>
<blockquote>
<p>如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？<br>这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。）<br>如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。</p>
</blockquote>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ufldl/">ufldl</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
        <p class="article-more-link">
          <a  href="/2015/07/23/softmax-regression-vs-logistic-regression/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-用matplotlib可视化来理解autoencoder中的sparse-penalty" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/23/用matplotlib可视化来理解autoencoder中的sparse-penalty/" class="article-date">
  	<time datetime="2015-07-23T13:48:50.000Z" itemprop="datePublished">2015-07-23</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/23/用matplotlib可视化来理解autoencoder中的sparse-penalty/">用matplotlib可视化来理解autoencoder中的sparse penalty</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近在看<a href="http://deeplearning.net/tutorial/dA.html#daa">Deep Learning Tutorials-Denoising Autoencoders</a>的内容，整个程序全部用theano和numpy写的，思路特别清晰。但是对于sparsity的限制没有感性的认识，一直觉得很玄幻。<br>今天就改了一下它的<a href="https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/dA.py">源码</a>，用matplotlib将hidden layer每个neuron输出的activation值用散点图的方式绘制出来，通过调节sparse_penalty参数进行对比。<br><img src="http://ww2.sinaimg.cn/large/901f9a6fgw1eucn84hf8qj20mk0h0jxm.jpg" alt="sparse 0.01"><br>图1：当sparse限制在0.01时<br>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearningtutorials/">deeplearningtutorials</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
        <p class="article-more-link">
          <a  href="/2015/07/23/用matplotlib可视化来理解autoencoder中的sparse-penalty/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-初识Learning-Rate-Schedule问题" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/21/初识Learning-Rate-Schedule问题/" class="article-date">
  	<time datetime="2015-07-21T13:34:03.000Z" itemprop="datePublished">2015-07-21</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/21/初识Learning-Rate-Schedule问题/">初识Learning Rate Schedule问题</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="背景">背景</h2><p>首先，上个图来看看。<br><img src="http://ww2.sinaimg.cn/large/901f9a6fgw1eua62ghcgvj20mj0g875y.jpg" alt="diffrenet learning rate compare"><br>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/neuralnetworksanddeeplearning/">neuralnetworksanddeeplearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
        <p class="article-more-link">
          <a  href="/2015/07/21/初识Learning-Rate-Schedule问题/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-logistic-sgd代码分析之Early-Stopping" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/15/logistic-sgd代码分析之Early-Stopping/" class="article-date">
  	<time datetime="2015-07-15T06:44:47.000Z" itemprop="datePublished">2015-07-15</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/15/logistic-sgd代码分析之Early-Stopping/">logistic_sgd代码分析之Early Stopping</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>暑期实习带我正式迈上了Deep Learning的道路，虽然DL目前来说“坑多路险”，还是感谢geetest的黄老板领我“入坑”。至少这半个月以来看了很多，学了很多，加入了一个新的大家庭。从刚开始的痛苦挣扎到思路逐渐清晰，收获还是不小的。<br>这篇是我学习<a href="http://deeplearning.net/tutorial/logreg.html">Deep Learning Tutorials-Logistic Regression</a>时对代码的理解，如果偏颇请多指正。</p>
<h2 id="Train_Model-Early_Stopping">Train Model-Early Stopping</h2><p>这一部分代码的重点是理解<a href="http://deeplearning.net/tutorial/gettingstarted.html#opt-early-stopping">Early-Stopping</a>的机制。</p>
<blockquote>
<p>Talk is cheap, show me your code.</p>
</blockquote>
<p>上代码》》》<br>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearningtutorials/">deeplearningtutorials</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
        <p class="article-more-link">
          <a  href="/2015/07/15/logistic-sgd代码分析之Early-Stopping/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-对Network类的代码分析" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/13/对Network类的代码分析/" class="article-date">
  	<time datetime="2015-07-13T05:53:05.000Z" itemprop="datePublished">2015-07-13</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/13/对Network类的代码分析/">对Network类的代码分析</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Neural Networks and Deep Learning 第一章学习笔记</p>
<p>感兴趣的请移步<a href="http://neuralnetworksanddeeplearning.com/chap1.html">Neural Networks and Deep Learning - Chapter 1</a></p>
<h2 id="SGD代码解析">SGD代码解析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span><br><span class="line">            test_data=None)</span>:</span></span><br><span class="line">        <span class="string">"""Train the neural network using mini-batch stochastic</span><br><span class="line">        gradient descent.  The "training_data" is a list of tuples</span><br><span class="line">        "(x, y)" representing the training inputs and the desired</span><br><span class="line">        outputs.  The other non-optional parameters are</span><br><span class="line">        self-explanatory.  If "test_data" is provided then the</span><br><span class="line">        network will be evaluated against the test data after each</span><br><span class="line">        epoch, and partial progress printed out.  This is useful for</span><br><span class="line">        tracking progress, but slows things down substantially."""</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                    j, self.evaluate(test_data), n_test)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/neuralnetworksanddeeplearning/">neuralnetworksanddeeplearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
        <p class="article-more-link">
          <a  href="/2015/07/13/对Network类的代码分析/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-Java访问权限" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/06/14/Java访问权限/" class="article-date">
  	<time datetime="2015-06-14T11:05:18.000Z" itemprop="datePublished">2015-06-14</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/14/Java访问权限/">Java访问权限</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="权限的基本介绍">权限的基本介绍</h2><h3 id="public">public</h3><h4 id="类">类</h4><p>当两个类不在同一个包中，public权限就显示出作用了。<br>如果一个类不是public的话，<code>外部包</code>无法对这个类进行访问。</p>
<h4 id="成员变量和函数">成员变量和函数</h4><p>如果想在包的外部调用某一个类的成员变量或者成员函数，那么这个成员必须是public，也就是必须是外部包可见的才能被访问，否则无法访问该成员。</p>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
        <p class="article-more-link">
          <a  href="/2015/06/14/Java访问权限/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-Java中的package" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/06/14/Java中的package/" class="article-date">
  	<time datetime="2015-06-13T16:09:05.000Z" itemprop="datePublished">2015-06-14</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/14/Java中的package/">Java中的package</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="什么是Java当中的软件包?">什么是Java当中的软件包?</h2><p>所谓的软件包<br>:    就是把类放在不同的文件夹下</p>
<p>相当于我们平时使用电脑时，有两个同名的文件，但是这两个文件我们都想保存，那么放在不同名的文件夹下，就可以了。对于Java软件包，使用的是同一个原理。</p>
<p>同时，可以理解为java的软件包<br>:    提供了命名空间</p>
<p>即规定了两个类可以重名，但是是两个不同的空间内可以重名。</p>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>
	</div>

      

      
        <p class="article-more-link">
          <a  href="/2015/06/14/Java中的package/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-由“哪些类型的值可以做python-dict的key”所引发的血案" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/05/30/由“哪些类型的值可以做python-dict的key”所引发的血案/" class="article-date">
  	<time datetime="2015-05-30T15:42:24.000Z" itemprop="datePublished">2015-05-30</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/30/由“哪些类型的值可以做python-dict的key”所引发的血案/">由“哪些类型的值可以做python dict的key”所引发的血案</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天遇到了这么一个问题，在C站的forum里小争论了一下</p>
<blockquote>
<p>list/dict/tuple/0/set([])/str中哪些类型能作为dict的key？</p>
</blockquote>
<p>平常较常用的dict的key值类型就是int啊，string啊，float啊，以前真没思考过这个问题，不过现在想来还是有点意思。</p>
<h2 id="什么是hashable？">什么是hashable？</h2><p>请看官方对hashable给出的<a href="https://docs.python.org/2.7/glossary.html#term-hashable">解释</a><br>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>
	</div>

      

      
        <p class="article-more-link">
          <a  href="/2015/05/30/由“哪些类型的值可以做python-dict的key”所引发的血案/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2015 Lan
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>